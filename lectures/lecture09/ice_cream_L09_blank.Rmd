---
title: "ice_cream_example"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(DT)
```

# Read in the data

* id: Identity of student
* female: Gender (0: male, 1: female)
* ice_cream: Favorite flavor (0: vanilla, 1: chocolate, 2: strawberry)
* video: score on video game
* puzzle: score on puzzle

```{r}

ice_cream <- read.csv("https://raw.githubusercontent.com/katiesevans/IGP_biostatistics/main/data/ice_cream.csv")

DT::datatable(ice_cream)

```

# Lecture 09

## Nonparametric tests

Split the original population of individuals into males and females. Perform a statistical analysis to determine if males are more likely to score higher on the puzzle than females.

What type of test is most appropriate here and why?

```{r}

# is puzzle normally distributed?


```

Perform the analysis suggested above. What is the conclusion?

```{r}

# split into two populations, male and female
males <- ice_cream %>%
    dplyr::filter(female == 0)

females <- ice_cream %>%
    dplyr::filter(female == 1)

# easy way - we can use wilcox.test for independent samples!





```

Does our data support the hypothesis that students scored the same on the puzzle and the video game?

What is the null and alternative hypothesis?

```{r}




```

First, use the sign test to get a rough estimate.

```{r}

# sign test - manually
# what is the difference in puzzle and video scores?


# how many positive values?


# how many negative values?


# NOTE! cant just do 200 - pos... why?


# binomial distribution: choose smaller of pos/neg values





```


Now, repeat the same test with the Wilcoxon signed-rank test. Since this test uses both rank and sign, it will likely give us a more appropriate estimate of the p-value.

```{r}

# repeat with wilcox test - paired samples (video and puzzle)




```

Why is it that the sign test and the signed-rank test produce such different results? To investigate this, let's look at the data.

```{r}

# the sign test only uses + and - but the signed rank test also adds in rank - using the
# absolute value of the difference...

# lets look at "diff"





```

## Bonus: Randomization

We talked about boot-strapping confidence intervals in lecture today and we wrote some code to do that... but what if we want to turn that into a function so we can change the confidence level or the data? Try to turn this code into a function:

```{r}
# generate random variable x
x <- rnorm(100)

# create variable to hold permutation means
perm_means <- c()
# do 10,000 permutations
for(i in 1:10000) {
    # sample from x with replacement (perm is same size as x)
    perm <- sample(x, replace = T)
    # calcualte mean and add to perm_means
    perm_means <- c(perm_means, mean(perm))
}

# get the middle 95% of the mean of the perms distribution
quantile(perm_means, c(0.025, 0.975))

```

